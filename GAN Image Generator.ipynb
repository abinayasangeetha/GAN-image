{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niELuhyWgz_u",
        "outputId": "27554ec0-0a34-4deb-b81d-d34f9318399e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 47.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.69MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.5MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.43MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Batch [0/938]  Loss D: 0.6959, Loss G: 0.7119\n",
            "Epoch [1/50] Batch [400/938]  Loss D: 0.4443, Loss G: 1.3666\n",
            "Epoch [1/50] Batch [800/938]  Loss D: 0.5397, Loss G: 1.3088\n",
            "Epoch [2/50] Batch [0/938]  Loss D: 0.4881, Loss G: 1.3272\n",
            "Epoch [2/50] Batch [400/938]  Loss D: 0.5340, Loss G: 0.7166\n",
            "Epoch [2/50] Batch [800/938]  Loss D: 0.4484, Loss G: 0.8659\n",
            "Epoch [3/50] Batch [0/938]  Loss D: 0.4789, Loss G: 1.4419\n",
            "Epoch [3/50] Batch [400/938]  Loss D: 0.5442, Loss G: 1.6351\n",
            "Epoch [3/50] Batch [800/938]  Loss D: 0.5254, Loss G: 2.0803\n",
            "Epoch [4/50] Batch [0/938]  Loss D: 0.3616, Loss G: 1.4333\n",
            "Epoch [4/50] Batch [400/938]  Loss D: 0.5379, Loss G: 0.5648\n",
            "Epoch [4/50] Batch [800/938]  Loss D: 0.4314, Loss G: 0.9286\n",
            "Epoch [5/50] Batch [0/938]  Loss D: 0.4761, Loss G: 0.7578\n",
            "Epoch [5/50] Batch [400/938]  Loss D: 0.4751, Loss G: 1.8608\n",
            "Epoch [5/50] Batch [800/938]  Loss D: 0.7294, Loss G: 0.3736\n",
            "Epoch [6/50] Batch [0/938]  Loss D: 0.4324, Loss G: 1.2577\n",
            "Epoch [6/50] Batch [400/938]  Loss D: 0.4929, Loss G: 1.6010\n",
            "Epoch [6/50] Batch [800/938]  Loss D: 0.6696, Loss G: 1.6269\n",
            "Epoch [7/50] Batch [0/938]  Loss D: 0.5111, Loss G: 0.9703\n",
            "Epoch [7/50] Batch [400/938]  Loss D: 0.5394, Loss G: 1.2091\n",
            "Epoch [7/50] Batch [800/938]  Loss D: 0.5175, Loss G: 0.7483\n",
            "Epoch [8/50] Batch [0/938]  Loss D: 0.4802, Loss G: 0.9300\n",
            "Epoch [8/50] Batch [400/938]  Loss D: 0.6843, Loss G: 0.4346\n",
            "Epoch [8/50] Batch [800/938]  Loss D: 0.5223, Loss G: 1.4300\n",
            "Epoch [9/50] Batch [0/938]  Loss D: 0.5193, Loss G: 1.3783\n",
            "Epoch [9/50] Batch [400/938]  Loss D: 0.6000, Loss G: 0.6294\n",
            "Epoch [9/50] Batch [800/938]  Loss D: 0.5018, Loss G: 0.8979\n",
            "Epoch [10/50] Batch [0/938]  Loss D: 0.5669, Loss G: 0.6445\n",
            "Epoch [10/50] Batch [400/938]  Loss D: 0.5789, Loss G: 0.7129\n",
            "Epoch [10/50] Batch [800/938]  Loss D: 0.6884, Loss G: 1.9056\n",
            "Epoch [11/50] Batch [0/938]  Loss D: 0.5180, Loss G: 1.4452\n",
            "Epoch [11/50] Batch [400/938]  Loss D: 0.5502, Loss G: 1.4751\n",
            "Epoch [11/50] Batch [800/938]  Loss D: 0.5955, Loss G: 0.5831\n",
            "Epoch [12/50] Batch [0/938]  Loss D: 0.5761, Loss G: 1.3186\n",
            "Epoch [12/50] Batch [400/938]  Loss D: 0.5342, Loss G: 0.8345\n",
            "Epoch [12/50] Batch [800/938]  Loss D: 0.5353, Loss G: 0.9709\n",
            "Epoch [13/50] Batch [0/938]  Loss D: 0.5850, Loss G: 1.6852\n",
            "Epoch [13/50] Batch [400/938]  Loss D: 0.5453, Loss G: 1.6112\n",
            "Epoch [13/50] Batch [800/938]  Loss D: 0.5033, Loss G: 1.0012\n",
            "Epoch [14/50] Batch [0/938]  Loss D: 0.5149, Loss G: 1.2063\n",
            "Epoch [14/50] Batch [400/938]  Loss D: 0.5488, Loss G: 0.8676\n",
            "Epoch [14/50] Batch [800/938]  Loss D: 0.5859, Loss G: 1.6073\n",
            "Epoch [15/50] Batch [0/938]  Loss D: 0.4823, Loss G: 1.1258\n",
            "Epoch [15/50] Batch [400/938]  Loss D: 0.5618, Loss G: 0.9644\n",
            "Epoch [15/50] Batch [800/938]  Loss D: 0.5955, Loss G: 1.5125\n",
            "Epoch [16/50] Batch [0/938]  Loss D: 0.5502, Loss G: 1.1376\n",
            "Epoch [16/50] Batch [400/938]  Loss D: 0.5976, Loss G: 1.5585\n",
            "Epoch [16/50] Batch [800/938]  Loss D: 0.5456, Loss G: 1.2381\n",
            "Epoch [17/50] Batch [0/938]  Loss D: 0.5399, Loss G: 1.0148\n",
            "Epoch [17/50] Batch [400/938]  Loss D: 0.5801, Loss G: 1.1872\n",
            "Epoch [17/50] Batch [800/938]  Loss D: 0.5459, Loss G: 0.8590\n",
            "Epoch [18/50] Batch [0/938]  Loss D: 0.5138, Loss G: 1.2716\n",
            "Epoch [18/50] Batch [400/938]  Loss D: 0.7446, Loss G: 0.4316\n",
            "Epoch [18/50] Batch [800/938]  Loss D: 0.7627, Loss G: 1.7803\n",
            "Epoch [19/50] Batch [0/938]  Loss D: 0.5833, Loss G: 0.9439\n",
            "Epoch [19/50] Batch [400/938]  Loss D: 0.5352, Loss G: 0.9274\n",
            "Epoch [19/50] Batch [800/938]  Loss D: 0.5509, Loss G: 1.2785\n",
            "Epoch [20/50] Batch [0/938]  Loss D: 0.6644, Loss G: 1.4153\n",
            "Epoch [20/50] Batch [400/938]  Loss D: 0.5673, Loss G: 0.9363\n",
            "Epoch [20/50] Batch [800/938]  Loss D: 0.6623, Loss G: 0.5713\n",
            "Epoch [21/50] Batch [0/938]  Loss D: 0.5532, Loss G: 0.8391\n",
            "Epoch [21/50] Batch [400/938]  Loss D: 0.5772, Loss G: 0.9308\n",
            "Epoch [21/50] Batch [800/938]  Loss D: 0.5671, Loss G: 0.8975\n",
            "Epoch [22/50] Batch [0/938]  Loss D: 0.5843, Loss G: 1.3877\n",
            "Epoch [22/50] Batch [400/938]  Loss D: 0.5681, Loss G: 1.4115\n",
            "Epoch [22/50] Batch [800/938]  Loss D: 0.5456, Loss G: 1.1273\n",
            "Epoch [23/50] Batch [0/938]  Loss D: 0.5586, Loss G: 0.8333\n",
            "Epoch [23/50] Batch [400/938]  Loss D: 0.5462, Loss G: 0.7855\n",
            "Epoch [23/50] Batch [800/938]  Loss D: 0.6162, Loss G: 1.2082\n",
            "Epoch [24/50] Batch [0/938]  Loss D: 0.5967, Loss G: 1.3391\n",
            "Epoch [24/50] Batch [400/938]  Loss D: 0.6027, Loss G: 1.6370\n",
            "Epoch [24/50] Batch [800/938]  Loss D: 0.5511, Loss G: 1.1300\n",
            "Epoch [25/50] Batch [0/938]  Loss D: 0.6188, Loss G: 0.6622\n",
            "Epoch [25/50] Batch [400/938]  Loss D: 0.5134, Loss G: 1.1274\n",
            "Epoch [25/50] Batch [800/938]  Loss D: 0.5948, Loss G: 1.0076\n",
            "Epoch [26/50] Batch [0/938]  Loss D: 0.5728, Loss G: 0.8167\n",
            "Epoch [26/50] Batch [400/938]  Loss D: 0.5367, Loss G: 1.2272\n",
            "Epoch [26/50] Batch [800/938]  Loss D: 0.5467, Loss G: 0.7949\n",
            "Epoch [27/50] Batch [0/938]  Loss D: 0.5921, Loss G: 0.9877\n",
            "Epoch [27/50] Batch [400/938]  Loss D: 0.5369, Loss G: 1.2115\n",
            "Epoch [27/50] Batch [800/938]  Loss D: 0.5796, Loss G: 0.7409\n",
            "Epoch [28/50] Batch [0/938]  Loss D: 0.5632, Loss G: 1.0988\n",
            "Epoch [28/50] Batch [400/938]  Loss D: 0.5598, Loss G: 0.8769\n",
            "Epoch [28/50] Batch [800/938]  Loss D: 0.5621, Loss G: 1.2446\n",
            "Epoch [29/50] Batch [0/938]  Loss D: 0.6058, Loss G: 0.7323\n",
            "Epoch [29/50] Batch [400/938]  Loss D: 0.5331, Loss G: 0.8433\n",
            "Epoch [29/50] Batch [800/938]  Loss D: 0.6037, Loss G: 1.6537\n",
            "Epoch [30/50] Batch [0/938]  Loss D: 0.6117, Loss G: 1.1922\n",
            "Epoch [30/50] Batch [400/938]  Loss D: 0.5845, Loss G: 0.6441\n",
            "Epoch [30/50] Batch [800/938]  Loss D: 0.7033, Loss G: 0.5131\n",
            "Epoch [31/50] Batch [0/938]  Loss D: 0.5753, Loss G: 0.9085\n",
            "Epoch [31/50] Batch [400/938]  Loss D: 0.5758, Loss G: 1.2727\n",
            "Epoch [31/50] Batch [800/938]  Loss D: 0.5745, Loss G: 1.1111\n",
            "Epoch [32/50] Batch [0/938]  Loss D: 0.5721, Loss G: 0.8024\n",
            "Epoch [32/50] Batch [400/938]  Loss D: 0.5580, Loss G: 0.9936\n",
            "Epoch [32/50] Batch [800/938]  Loss D: 0.7103, Loss G: 1.4497\n",
            "Epoch [33/50] Batch [0/938]  Loss D: 0.5719, Loss G: 0.7630\n",
            "Epoch [33/50] Batch [400/938]  Loss D: 0.5539, Loss G: 1.5195\n",
            "Epoch [33/50] Batch [800/938]  Loss D: 0.5299, Loss G: 1.1182\n",
            "Epoch [34/50] Batch [0/938]  Loss D: 0.4996, Loss G: 1.1548\n",
            "Epoch [34/50] Batch [400/938]  Loss D: 0.5776, Loss G: 1.3222\n",
            "Epoch [34/50] Batch [800/938]  Loss D: 0.6176, Loss G: 1.1258\n",
            "Epoch [35/50] Batch [0/938]  Loss D: 0.5604, Loss G: 0.7905\n",
            "Epoch [35/50] Batch [400/938]  Loss D: 0.6069, Loss G: 1.1320\n",
            "Epoch [35/50] Batch [800/938]  Loss D: 0.5272, Loss G: 0.9755\n",
            "Epoch [36/50] Batch [0/938]  Loss D: 0.5688, Loss G: 1.1030\n",
            "Epoch [36/50] Batch [400/938]  Loss D: 0.5244, Loss G: 1.2488\n",
            "Epoch [36/50] Batch [800/938]  Loss D: 0.5903, Loss G: 1.2517\n",
            "Epoch [37/50] Batch [0/938]  Loss D: 0.5022, Loss G: 1.0058\n",
            "Epoch [37/50] Batch [400/938]  Loss D: 0.6257, Loss G: 0.7133\n",
            "Epoch [37/50] Batch [800/938]  Loss D: 0.6069, Loss G: 0.8132\n",
            "Epoch [38/50] Batch [0/938]  Loss D: 0.5254, Loss G: 1.1142\n",
            "Epoch [38/50] Batch [400/938]  Loss D: 0.5500, Loss G: 1.1627\n",
            "Epoch [38/50] Batch [800/938]  Loss D: 0.6156, Loss G: 0.7439\n",
            "Epoch [39/50] Batch [0/938]  Loss D: 0.6200, Loss G: 0.6363\n",
            "Epoch [39/50] Batch [400/938]  Loss D: 0.5236, Loss G: 1.0059\n",
            "Epoch [39/50] Batch [800/938]  Loss D: 0.5558, Loss G: 0.9570\n",
            "Epoch [40/50] Batch [0/938]  Loss D: 0.5503, Loss G: 1.2139\n",
            "Epoch [40/50] Batch [400/938]  Loss D: 0.5423, Loss G: 1.0545\n",
            "Epoch [40/50] Batch [800/938]  Loss D: 0.5552, Loss G: 1.2391\n",
            "Epoch [41/50] Batch [0/938]  Loss D: 0.5276, Loss G: 1.2246\n",
            "Epoch [41/50] Batch [400/938]  Loss D: 0.5415, Loss G: 0.8771\n",
            "Epoch [41/50] Batch [800/938]  Loss D: 0.6257, Loss G: 1.4723\n",
            "Epoch [42/50] Batch [0/938]  Loss D: 0.5807, Loss G: 0.7088\n",
            "Epoch [42/50] Batch [400/938]  Loss D: 0.6106, Loss G: 1.2329\n",
            "Epoch [42/50] Batch [800/938]  Loss D: 0.5598, Loss G: 1.4189\n",
            "Epoch [43/50] Batch [0/938]  Loss D: 0.5400, Loss G: 0.9807\n",
            "Epoch [43/50] Batch [400/938]  Loss D: 0.5518, Loss G: 1.2957\n",
            "Epoch [43/50] Batch [800/938]  Loss D: 0.5440, Loss G: 1.0676\n",
            "Epoch [44/50] Batch [0/938]  Loss D: 0.5747, Loss G: 0.8470\n",
            "Epoch [44/50] Batch [400/938]  Loss D: 0.5407, Loss G: 1.3327\n",
            "Epoch [44/50] Batch [800/938]  Loss D: 0.5189, Loss G: 1.4041\n",
            "Epoch [45/50] Batch [0/938]  Loss D: 0.5357, Loss G: 1.3454\n",
            "Epoch [45/50] Batch [400/938]  Loss D: 0.5641, Loss G: 0.8512\n",
            "Epoch [45/50] Batch [800/938]  Loss D: 0.5699, Loss G: 1.1249\n",
            "Epoch [46/50] Batch [0/938]  Loss D: 0.5823, Loss G: 0.8469\n",
            "Epoch [46/50] Batch [400/938]  Loss D: 0.5965, Loss G: 1.1276\n",
            "Epoch [46/50] Batch [800/938]  Loss D: 0.5478, Loss G: 0.7678\n",
            "Epoch [47/50] Batch [0/938]  Loss D: 0.6106, Loss G: 1.4727\n",
            "Epoch [47/50] Batch [400/938]  Loss D: 0.6446, Loss G: 1.5529\n",
            "Epoch [47/50] Batch [800/938]  Loss D: 0.5548, Loss G: 1.6393\n",
            "Epoch [48/50] Batch [0/938]  Loss D: 0.5225, Loss G: 1.3120\n",
            "Epoch [48/50] Batch [400/938]  Loss D: 0.5519, Loss G: 1.0327\n",
            "Epoch [48/50] Batch [800/938]  Loss D: 0.5915, Loss G: 0.7207\n",
            "Epoch [49/50] Batch [0/938]  Loss D: 0.5570, Loss G: 0.8311\n",
            "Epoch [49/50] Batch [400/938]  Loss D: 0.5438, Loss G: 1.0321\n",
            "Epoch [49/50] Batch [800/938]  Loss D: 0.5277, Loss G: 0.8845\n",
            "Epoch [50/50] Batch [0/938]  Loss D: 0.5466, Loss G: 1.2117\n",
            "Epoch [50/50] Batch [400/938]  Loss D: 0.4792, Loss G: 1.2361\n",
            "Epoch [50/50] Batch [800/938]  Loss D: 0.5421, Loss G: 1.3748\n",
            "✅ Training complete. Check 'generated_images' folder.\n"
          ]
        }
      ],
      "source": [
        "# GAN in PyTorch - MNIST dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Directories\n",
        "os.makedirs(\"generated_images\", exist_ok=True)\n",
        "\n",
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "img_shape = (1, 28, 28)\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# Dataset\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\"data\", train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        return img.view(z.size(0), *img_shape)\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(784, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        return self.model(img_flat)\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "        real_imgs = imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Real and Fake labels\n",
        "        real = torch.ones(batch_size, 1).to(device)\n",
        "        fake = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        z = torch.randn(batch_size, latent_dim).to(device)\n",
        "        gen_imgs = generator(z)\n",
        "        g_loss = criterion(discriminator(gen_imgs), real)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_loss = criterion(discriminator(real_imgs), real)\n",
        "        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        if i % 400 == 0:\n",
        "            print(f\"Epoch [{epoch}/{epochs}] Batch [{i}/{len(dataloader)}]  Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}\")\n",
        "\n",
        "    # Save sample images\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(64, latent_dim).to(device)\n",
        "            gen_imgs = generator(z)\n",
        "            grid = make_grid(gen_imgs, nrow=8, normalize=True)\n",
        "            save_image(grid, f\"generated_images/epoch_{epoch}.png\")\n",
        "\n",
        "print(\"Training complete. Check 'generated_images' folder.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"gan_outputs\", 'zip', \"generated_images\")\n",
        "from google.colab import files\n",
        "files.download(\"gan_outputs.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sF5Y7LGlseD_",
        "outputId": "9e5b3fad-1b8d-4428-8c92-a6cc4ffac8df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_41e29520-281f-4508-93e3-4b45d8cc776f\", \"gan_outputs.zip\", 276053)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(generator.state_dict(), \"generator.pth\")\n"
      ],
      "metadata": {
        "id": "z5xNEalg2-MG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"generator.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xNh_7mRK3B4q",
        "outputId": "c0039df7-73cd-4489-b0c3-bb9d5675b538"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2b0604ee-5ea4-4d02-bf51-a3c75682b283\", \"generator.pth\", 2337228)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VUPA2SAa3DTK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}